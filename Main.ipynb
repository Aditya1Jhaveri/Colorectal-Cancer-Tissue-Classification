{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized images done\n"
     ]
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# input_base_folder = 'CRC-Dataset'\n",
    "# output_base_folder = 'CRC-Dataset_resized'\n",
    "\n",
    "# # List of class folders\n",
    "# class_folders = os.listdir(input_base_folder)\n",
    "\n",
    "# # Iterate through class folders\n",
    "# for class_folder in class_folders:\n",
    "#     input_folder = os.path.join(input_base_folder, class_folder)\n",
    "#     output_folder = os.path.join(output_base_folder, class_folder)\n",
    "\n",
    "#     # Create the output folder if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#     # Resize images and save them to the output folder\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         image_path = os.path.join(input_folder, filename)\n",
    "\n",
    "#         # Load the image using PIL\n",
    "#         image = Image.open(image_path)\n",
    "\n",
    "#         # Resize the image to your desired size (e.g., 224x224)\n",
    "#         target_size = (224, 224)\n",
    "#         image = image.resize(target_size)\n",
    "\n",
    "#         # Save the resized image to the output folder\n",
    "#         save_path = os.path.join(output_folder, filename)\n",
    "#         image.save(save_path)\n",
    "\n",
    "# print(\"Resized images done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2551a519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise removal and PNG conversion completed.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_dir = \"CRC-Dataset\"\n",
    "\n",
    "# Detect classes automatically\n",
    "classes = [class_name for class_name in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, class_name))]\n",
    "\n",
    "# Define NLM filter function (using scikit-image)\n",
    "def nlm_filter(image):\n",
    "    # Convert to float [0,1]\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    sigma_est = np.mean(estimate_sigma(image, channel_axis=-1)) \n",
    "    patch_kw = dict(patch_size=5, patch_distance=6, channel_axis=-1) \n",
    "    denoised = denoise_nl_means(image, h=1.15 * sigma_est, fast_mode=True, **patch_kw)\n",
    "    return img_as_ubyte(denoised)  # back to 8-bit\n",
    "\n",
    "# Iterate through each class\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    output_dir = os.path.join(\"CRC-Dataset_NLM\", class_name, f\"{class_name}_nlm\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Accept tif, tiff, bmp, jpg, png\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, filename)\n",
    "        for filename in os.listdir(class_dir)\n",
    "        if filename.lower().endswith((\".tif\"))\n",
    "    ]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Load with Pillow\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        # Apply NLM filter\n",
    "        filtered_image = nlm_filter(image_np)\n",
    "\n",
    "        # Save as PNG\n",
    "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
    "        output_file = os.path.join(output_dir, f\"{filename}.png\")\n",
    "        Image.fromarray(filtered_image).save(output_file)\n",
    "\n",
    "print(\"Noise removal and PNG conversion completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aaeb989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Class 'ADI_nlm': 1338 source images.\n",
      "[INFO] Class 'BACK_nlm': 847 source images.\n",
      "[INFO] Class 'DEB_nlm': 339 source images.\n",
      "[INFO] Class 'LYM_nlm': 634 source images.\n",
      "[INFO] Class 'MUC_nlm': 1035 source images.\n",
      "[INFO] Class 'MUS_nlm': 592 source images.\n",
      "[INFO] Class 'NORM_nlm': 741 source images.\n",
      "[INFO] Class 'STR_nlm': 421 source images.\n",
      "[INFO] Class 'TUM_nlm': 1233 source images.\n",
      "Done. Inputs processed: 7180, augmented images saved: 28720.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "\n",
    "# ---- Config ----\n",
    "base_input_folder = 'CRC-Dataset_NLM'\n",
    "base_output_folder = 'CRC-NLM_Augmentation'\n",
    "IMG_SIZE = (224, 224)\n",
    "AUG_PER_IMAGE = 4\n",
    "USE_VERTICAL_FLIP = True\n",
    "\n",
    "# Accept common formats\n",
    "ALLOWED_EXTS = ('.png',) \n",
    "\n",
    "class_folders = [\n",
    "    f for f in os.listdir(base_input_folder)\n",
    "    if os.path.isdir(os.path.join(base_input_folder, f))\n",
    "]\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.05,\n",
    "    zoom_range=0.1,\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=USE_VERTICAL_FLIP,\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "os.makedirs(base_output_folder, exist_ok=True)\n",
    "\n",
    "total_in = 0\n",
    "total_out = 0\n",
    "\n",
    "for class_folder in class_folders:\n",
    "    input_folder = os.path.join(base_input_folder, class_folder)\n",
    "    output_folder = os.path.join(base_output_folder, class_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Collect valid input files\n",
    "    files = [fn for fn in os.listdir(input_folder)\n",
    "             if fn.lower().endswith(ALLOWED_EXTS)]\n",
    "\n",
    "    if not files:\n",
    "        print(f\"[WARN] No images found for class '{class_folder}' in {input_folder} \"\n",
    "              f\"(allowed: {ALLOWED_EXTS}).\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[INFO] Class '{class_folder}': {len(files)} source images.\")\n",
    "\n",
    "    for filename in files:\n",
    "        in_path = os.path.join(input_folder, filename)\n",
    "        total_in += 1\n",
    "        try:\n",
    "            # Load with consistent mode/size\n",
    "            img = load_img(in_path, color_mode='rgb', target_size=IMG_SIZE)\n",
    "            arr = img_to_array(img)\n",
    "            arr = np.expand_dims(arr, axis=0)  # (1, H, W, C)\n",
    "\n",
    "            # Create generator (1 at a time, avoids waste)\n",
    "            flow = datagen.flow(arr, batch_size=1)\n",
    "\n",
    "            stem = os.path.splitext(filename)[0]\n",
    "            for i in range(AUG_PER_IMAGE):\n",
    "                batch = next(flow)  # (1, H, W, C), scaled [0,1]\n",
    "                aug = (batch[0] * 255).astype('uint8')  # back to uint8 for saving\n",
    "                out_img = Image.fromarray(aug)\n",
    "                out_path = os.path.join(output_folder, f'{stem}_aug_{i}.png')\n",
    "                out_img.save(out_path)\n",
    "                total_out += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'[WARN] Skipping {in_path}: {e}')\n",
    "\n",
    "print(f\"Done. Inputs processed: {total_in}, augmented images saved: {total_out}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4b5b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 2732 images. Remaining: 2200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def reduce_images(folder_path, target_count):\n",
    "    # List all files in the folder\n",
    "    all_files = [f for f in os.listdir(folder_path) \n",
    "                 if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    current_count = len(all_files)\n",
    "    \n",
    "    if current_count <= target_count:\n",
    "        print(f\"No deletion needed. Current images: {current_count}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate how many to delete\n",
    "    delete_count = current_count - target_count\n",
    "    \n",
    "    # Randomly select files to delete\n",
    "    files_to_delete = random.sample(all_files, delete_count)\n",
    "    \n",
    "    for f in files_to_delete:\n",
    "        os.remove(os.path.join(folder_path, f))\n",
    "    \n",
    "    print(f\"Deleted {delete_count} images. Remaining: {target_count}\")\n",
    "\n",
    "\n",
    "reduce_images(\"CRC-NLM_Augmentation/TUM_nlm\", 2200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb36e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the directories for train and test datasets\n",
    "train_dir = 'New NLM Augmentation/train'  # Replace with your train dataset directory\n",
    "test_dir = 'New NLM Augmentation/test'    # Replace with your test dataset directory\n",
    "output_dir = 'NEW_DATASET_SIZE'  # Replace with the directory where you want to store the merged dataset\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List the classes in the train dataset (assuming each subdirectory corresponds to a class)\n",
    "classes = os.listdir(train_dir)\n",
    "\n",
    "# Merge train and test datasets by copying files to the output directory\n",
    "for class_name in classes:\n",
    "    train_class_dir = os.path.join(train_dir, class_name)\n",
    "    test_class_dir = os.path.join(test_dir, class_name)\n",
    "    output_class_dir = os.path.join(output_dir, class_name)\n",
    "    \n",
    "    # Create the class directory in the merged dataset if it doesn't exist\n",
    "    os.makedirs(output_class_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy images from the train dataset\n",
    "    train_files = os.listdir(train_class_dir)\n",
    "    for file in train_files:\n",
    "        src_path = os.path.join(train_class_dir, file)\n",
    "        dst_path = os.path.join(output_class_dir, file)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    # Copy images from the test dataset\n",
    "    test_files = os.listdir(test_class_dir)\n",
    "    for file in test_files:\n",
    "        src_path = os.path.join(test_class_dir, file)\n",
    "        dst_path = os.path.join(output_class_dir, file)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"Merging complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f774dd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herlev dataset split into train, validation, and test sets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Set your data directory\n",
    "data_dir = 'NEW_DATASET_SIZE'  # Replace with the path to your Herlev dataset\n",
    "\n",
    "# Define the directory names for the splits\n",
    "train_dir = 'NEW_DATASET/train'\n",
    "val_dir = 'NEW_DATASET/validation'\n",
    "test_dir = 'NEW_DATASET/test'\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Define the ratio for splitting (e.g., 70% train, 15% validation, 15% test)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.18\n",
    "test_ratio = 0.12\n",
    "\n",
    "# Loop through each class in the dataset\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        # List all the files in the class directory\n",
    "        files = os.listdir(class_dir)\n",
    "        random.shuffle(files)  # Shuffle the files\n",
    "        \n",
    "        num_files = len(files)\n",
    "        \n",
    "        # Calculate the number of samples for each split\n",
    "        num_train = int(train_ratio * num_files)\n",
    "        num_val = int(val_ratio * num_files)\n",
    "        num_test = num_files - num_train - num_val\n",
    "        \n",
    "        # Create subdirectories for each split\n",
    "        class_train_dir = os.path.join(train_dir, class_name)\n",
    "        class_val_dir = os.path.join(val_dir, class_name)\n",
    "        class_test_dir = os.path.join(test_dir, class_name)\n",
    "        \n",
    "        os.makedirs(class_train_dir, exist_ok=True)\n",
    "        os.makedirs(class_val_dir, exist_ok=True)\n",
    "        os.makedirs(class_test_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy files to respective splits\n",
    "        for i, file in enumerate(files):\n",
    "            src_path = os.path.join(class_dir, file)\n",
    "            if i < num_train:\n",
    "                dst_path = os.path.join(class_train_dir, file)\n",
    "            elif i < num_train + num_val:\n",
    "                dst_path = os.path.join(class_val_dir, file)\n",
    "            else:\n",
    "                dst_path = os.path.join(class_test_dir, file)\n",
    "            \n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"Herlev dataset split into train, validation, and test sets.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
